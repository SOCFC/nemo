#!/usr/bin/env python

"""

nemo driver script: for filtering maps and finding clusters

"""

import sys
#print("Running under python: %s" % (sys.version))
import os
import datetime
from nemo import *
import nemo
import argparse
import astropy
import astropy.table as atpy
import astropy.io.fits as pyfits
from astLib import astWCS
import numpy as np
import pylab
import pickle
import types
import yaml
import IPython
pylab.matplotlib.interactive(False)
plotSettings.update_rcParams()

#------------------------------------------------------------------------------------------------------------
if __name__ == '__main__':

    parser=argparse.ArgumentParser("nemo")
    parser.add_argument("configFileName", help="""A .yml configuration file.""")
    parser.add_argument("-M", "--mpi", dest="MPIEnabled", action="store_true", help="""Enable MPI. If you
                        want to use this, run using something like: mpiexec --np 4 nemo ...""", 
                        default = False)
    args=parser.parse_args()
    
    parDictFileName=args.configFileName
    config=startUp.NemoConfig(parDictFileName, MPIEnabled = args.MPIEnabled)
    
    imageDict=pipelines.filterMapsAndMakeCatalogs(config)

    # Optional position recovery test (how well do we recovery positions as fn. of S/N)
    if 'positionRecoveryTest' in config.parDict.keys() and config.parDict['positionRecoveryTest'] == True:
        maps.positionRecoveryTest(config, imageDict)
        
    # Q function (filter mismatch) - if needed options have been given
    # We may as well do this here to save having to run nemoMass separately (though we still can...)
    # (it's the Q calc in nemoMass that takes time - but it's only a couple of min per field in parallel)
    if 'photFilter' in config.parDict.keys() and config.parDict['photFilter'] is not None:
        tckQFitDict=signals.fitQ(config)
    
    # Estimate of contamination from running cluster finding over inverted map
    if 'estimateContaminationFromInvertedMaps' in list(config.parDict.keys()) and config.parDict['estimateContaminationFromInvertedMaps'] == True:
        conTabDict=maps.estimateContaminationFromInvertedMaps(config, imageDict)
    else:
        conTabDict={}
        
    # Estimate of contamination by generating a fake sky with noise, and running detection algorithm over it
    # Ultimately we want this and the above ^^^ to appear on the same plot for comparison
    if 'estimateContaminationFromSkySim' in list(config.parDict.keys()) and config.parDict['estimateContaminationFromSkySim'] == True:
        skySimConTabDict=maps.estimateContaminationFromSkySim(config, imageDict) 
    else:
        skySimConTabDict={}
    
    # This just combines inverted maps contamination results and skySim (under different keys)
    # So we only feed one dictionary into the plotting routine (see below)
    for k in list(skySimConTabDict.keys()):
        conTabDict[k]=skySimConTabDict[k]
        
    # MPI: gather together and merge all of the catalogs from each process, and contamination test results
    if config.MPIEnabled == True:
        optimalCatalogList=config.comm.gather(imageDict['optimalCatalog'], root = 0)
        conKeysList=list(conTabDict.keys())
        conTabDictList=config.comm.gather(conTabDict, root = 0)
        if config.rank != 0:
            assert optimalCatalogList is None
            print("... MPI rank %d finished ..." % (config.rank))
            sys.exit()
        else:
            print("... gathering catalogs ...")
            toStack=[]  # We sometimes return [] if no objects found - we can't vstack those
            for collectedTab in optimalCatalogList:
                if type(collectedTab) == astropy.table.table.Table:
                    toStack.append(collectedTab)
            optimalCatalog=atpy.vstack(toStack)
            print("... gathering and averaging contamination estimates ...")
            avConTabDict={}
            for k in conKeysList:
                tabList=[]
                avTab=None
                tabCount=0
                for tabDict in conTabDictList:
                    tab=tabDict[k]
                    if type(avTab) == type(None):
                        avTab=atpy.Table()
                        for colName in list(tab.keys()):
                            avTab.add_column(atpy.Column(np.zeros(len(tab)), colName))
                    for colName in list(avTab.keys()):
                        avTab[colName]=avTab[colName]+tab[colName]
                    tabCount=tabCount+1
                for colName in list(avTab.keys()):
                    avTab[colName]=avTab[colName]/float(tabCount)
                mask=np.greater(avTab['cumSumSimCandidates'], 0)
                avTab['cumContamination']=atpy.Column(np.zeros(len(avTab)), 'cumContamination')
                avTab['cumContamination'][mask]=avTab['cumSumSimCandidates'][mask]/avTab['cumSumRealCandidates'][mask]
                avConTabDict[k]=avTab
            conTabDictList=avConTabDict
            # And write average as .fits table(s)
            # (if we didn't run under MPI, already averaged)
            for k in list(avConTabDict.keys()):
                fitsOutFileName=config.diagnosticsDir+os.path.sep+"%s_contaminationEstimate_%s.fits" % (k, "_average")
                if os.path.exists(fitsOutFileName) == True:
                    os.remove(fitsOutFileName)
                conTab=avConTabDict[k]
                conTab.write(fitsOutFileName) 
    else:
        optimalCatalog=imageDict['optimalCatalog']
    
    # Plot contamination together
    if conTabDict != {}:
        maps.plotContamination(conTabDict, config.diagnosticsDir)           
    
    # Strip out duplicates (this is necessary when run in tileDeck mode under MPI)
    if len(optimalCatalog) > 0:
        optimalCatalog, numDuplicatesFound, names=catalogs.removeDuplicates(optimalCatalog)
    # We can do this twice as a sanity check... not necessary if we add a proper test elsewhere
    #optimalCatalog, numDuplicatesFound, names=catalogs.removeDuplicates(optimalCatalog)
    #assert(numDuplicatesFound == 0)

    optimalCatalogFileName=config.rootOutDir+os.path.sep+"%s_optimalCatalog.csv" % (os.path.split(config.rootOutDir)[-1])           
    catalogs.writeCatalog(optimalCatalog, optimalCatalogFileName, constraintsList = ["SNR > 0.0"])
    catalogs.writeCatalog(optimalCatalog, optimalCatalogFileName.replace(".csv", ".fits"), constraintsList = ["SNR > 0.0"])
            
    addInfo=[{'key': 'SNR', 'fmt': '%.1f'}]
    catalogs.catalog2DS9(optimalCatalog, optimalCatalogFileName.replace(".csv", ".reg"),
                         constraintsList = ["SNR > 0.0"], \
                         addInfo = addInfo, color = "cyan") 
    
    # Stitch together map tiles - these are 'quicklook' images (downsampled by factor 4 in resolution)
    # The stitchTiles routine will only write output if there are multiple maps matching the file pattern
    # RMS
    if 'makeQuickLookMaps' in config.parDict.keys() and config.parDict['makeQuickLookMaps'] == True:
        maps.stitchTiles(config.diagnosticsDir+os.path.sep+"RMSMap*.fits", 
                        config.diagnosticsDir+os.path.sep+"quicklook_RMSMap.fits", 
                        config.quicklookWCS, config.quicklookShape, fluxRescale = config.quicklookScale)
        # S/N maps at reference filter scale
        if config.parDict['photFilter'] is not None:
            maps.stitchTiles(config.filteredMapsDir+os.path.sep+"%s*SNMap.fits" % (config.parDict['photFilter']), 
                            config.filteredMapsDir+os.path.sep+"quicklook_%s_SNMap.fits" % (config.parDict['photFilter']), 
                            config.quicklookWCS, config.quicklookShape, fluxRescale = config.quicklookScale)    

    
    

#!/usr/bin/env python

"""

Calculate masses of clusters detected by nemo

Requires 'massOptions' in nemo config file

Run this after filtering / detecting objects in a map with nemo itself

Can be used to obtain 'forced photometry', i.e., mass estimates for objects in redshiftCatalog
(for, e.g., optical stacking)

"""

import os
import sys
import numpy as np
import pylab as plt
import astropy.table as atpy
from astLib import *
from scipy import stats
from scipy import interpolate
from nemo import catalogTools
from nemo import simsTools
from nemo import mapTools
from nemo import mapFilters
from nemo import MockSurvey
from nemo import photometry
from nemo import startUp
import astropy.io.fits as pyfits
import time
import yaml
import IPython
plt.matplotlib.interactive(False)

#------------------------------------------------------------------------------------------------------------
def addForcedPhotometry(forcedTab, parDict, unfilteredMapsDict, extNames, diagnosticsDir):
    """Given a tab with (minimum) name, RADeg, decDeg, redshift columns, add forced photometry columns so we
    can estimate SZ masses.
    
    """
    
    print(">>> Doing forced photometry ...")
    
    # Filter maps - but should already have been done, this is just for easy set up
    photFilterLabel=parDict['photometryOptions']['photFilter']
    photFilterList=[]
    for filterDict in parDict['mapFilters']:
        if filterDict['label'] == photFilterLabel:
            photFilterList.append(filterDict)
    imageDict=mapFilters.filterMaps(parDict['unfilteredMaps'], photFilterList, extNames = extNames, rootOutDir = rootOutDir)
    
    # Re-jig the catalog into the appropriate format before feeding through photometry routine
    catalog=[]
    for extName in extNames:
        # We need this to check if objects in the catalog are actually in the footprint
        areaMaskImg=pyfits.open(diagnosticsDir+os.path.sep+"areaMask#%s.fits" % (extName))
        areaMask=areaMaskImg[0].data
        wcs=astWCS.WCS(areaMaskImg[0].header, mode = 'pyfits')
        for row in forcedTab:
            objDict={}
            for key in list(forcedTab.keys()):
                objDict[key]=row[key]
            objDict['template']=photFilterLabel+"#"+extName
            objDict['SNR']=1.0  # Just to keep photometry routine happy; not used for masses
            objDict['redshift']=row['redshift']
            if 'redshiftErr' in forcedTab.keys():
                objDict['redshiftErr']=row['redshiftErr']
            else:
                objDict['redshiftErr']=0.0
            x, y=wcs.wcs2pix(objDict['RADeg'], objDict['decDeg'])
            if x > 0 and y > 0 and x < areaMask.shape[1] and y < areaMask.shape[0]:
                if areaMask[int(round(y)), int(round(x))] == 1:
                    objDict['x']=x
                    objDict['y']=y
                    catalog.append(objDict)
    imageDict[photFilterLabel+"#"+extName]['catalog']=catalog
    photometry.measureFluxes(imageDict, parDict['photometryOptions'], diagnosticsDir, unfilteredMapsDict = parDict['unfilteredMaps'])
    catalogTools.mergeCatalogs(imageDict)
    
    # No cuts here, because if we're using this mode, the objects almost certainly weren't detected...
    catalogTools.makeOptimalCatalog(imageDict, [])  
    photometry.addFreqWeightsToCatalog(imageDict, parDict['photometryOptions'], diagnosticsDir)

    # Make into an atpy Table
    # NOTE: we're going to extract the tile name from 'template'... not very elegant - should just write in the nemo catalog
    wantedKeys=['name', 'RADeg', 'decDeg', 'fixed_SNR', 'fixed_y_c', 'fixed_err_y_c', 'redshift', 'redshiftErr', 'template']
    for key in imageDict['optimalCatalog'][0].keys():
        if key.find("fixed_y_c_weight") != -1:
            wantedKeys.append(key)
    tab=atpy.Table()
    for extName in extNames:
        catalog=imageDict['optimalCatalog']
        for key in wantedKeys:
            arr=[]
            for objDict in catalog:
                if key in list(objDict.keys()):
                    arr.append(objDict[key])
                else:
                    arr.append(0)
            tab.add_column(atpy.Column(np.array(arr), key))
    tab=tab[np.where(tab['redshift'] != 0)]

    # Flag things with -ve y0~, which we can't do anything with (unless we add stacking / averaging mode)
    for row in tab:
        if row['fixed_y_c'] < 0:
            print("... %s has fixed_y_c < 0 (and is removed) ..." % (row['name']))
    tab=tab[np.where(tab['fixed_y_c'] > 0)]

    return tab

#------------------------------------------------------------------------------------------------------------
# Main
if len(sys.argv) < 3:
    print("Run: % nemoMass <.yml config file> <catalog.fits | default>")
else:
    
    parDictFileName=sys.argv[1]
    catFileName=sys.argv[2]
    
    parDict, rootOutDir, filteredMapsDir, diagnosticsDir, unfilteredMapsDictList, extNames, comm, rank, size=startUp.startUp(parDictFileName)
    
    massOptions=parDict['massOptions']
    
    forcedPhotometry=False
    
    # Load the nemo catalog and match against the z catalog
    if catFileName == 'default':
        optimalCatalogFileName=rootOutDir+os.path.sep+"%s_optimalCatalog.fits" % (os.path.split(rootOutDir)[-1])           
        nemoTab=atpy.Table().read(optimalCatalogFileName)
        zTab=atpy.Table().read(massOptions['redshiftCatalog'])
        if massOptions['forcedPhotometry'] == False:
            # Remove any freq weight columns from redshift tab before we start
            for key in zTab.keys():
                if key.find("fixed_y_c_weight") != -1:
                    zTab.remove_column(key)
            tab=atpy.join(nemoTab, zTab, 'name')
            colsToRemove=[]
            for k in list(tab.keys()):
                if k[-2:] == '_2':
                    colsToRemove.append(k)
            tab.remove_columns(colsToRemove)
            # NOTE: we're going to extract the tile name from 'template'... not very elegant - should just write in the nemo catalog
            colsToKeep=['name', 'RADeg', 'decDeg', 'fixed_SNR', 'fixed_y_c', 'fixed_err_y_c', 'redshift', 'redshiftErr', 'template']
            for k in colsToKeep:
                if k+"_1" in list(tab.keys()):
                    tab.rename_column(k+"_1", k)
            colsToRemove=[]
            for k in list(tab.keys()):
                if k not in colsToKeep and k.find('fixed_y_c_weight') == -1:
                    colsToRemove.append(k)
            tab.remove_columns(colsToRemove)
            outFileName=optimalCatalogFileName.replace("_optimalCatalog.fits", "_M500.fits")
    
        elif massOptions['forcedPhotometry'] == True:
            # Supplying this option in massOptions is no longer strictly necessary (see below)
            forcedPhotometry=True

    else:
        
        # Load another catalog (e.g., a mock, for testing)
        optimalCatalogFileName=catFileName
        tab=atpy.Table().read(optimalCatalogFileName)
        outFileName=catFileName.replace(".fits", "_M500.fits")
        
        # Enter forced photometry mode if we can't find the columns we need
        keysNeeded=['fixed_y_c', 'fixed_err_y_c']
        forcedPhotometry=False
        for key in keysNeeded:
            if key not in tab.keys():
                forcedPhotometry=True

    # Q function (filter mismatch) options
    # NOTE: this should be calculated for the fixed, fiducial cosmology used for object detection
    # NOTE: Q calculation is the only bit of this code that uses MPI - rank != 0 MPI jobs killed after fitQ
    if massOptions['Q'] == 'H13':
        raise Exception('H13 option depreciated - need to replace polynomial with spline fit')
        tckQFit=simsTools.getQCoeffsH13()
    elif massOptions['Q'] == 'fit':
        tckQFitDict=simsTools.fitQ(parDict, diagnosticsDir, filteredMapsDir, extNames = extNames, 
                                    MPIEnabled = parDict['useMPI'], rank = rank, comm = comm)
    else:
        raise Exception("didn't understand choice of Q function in massOptions")
    
    # Forced photometry (if enabled) - modifying table in place
    # NOTE: Move this up if/when we make it run under MPI
    if forcedPhotometry == True:
        tab=addForcedPhotometry(tab, parDict, unfilteredMapsDictList, extNames, diagnosticsDir)

    # Optional fixed SNR cut
    if 'fixedSNRCut' in list(massOptions.keys()):
        tab=tab[np.where(tab['fixed_SNR'] > massOptions['fixedSNRCut'])]
    
    # Set cosmological parameters for e.g. E(z) calc if these are set in .par file
    # We set them after the Q calc, because the Q calc needs to be for the fiducial cosmology
    # (OmegaM0 = 0.3, OmegaL0 = 0.7, H0 = 70 km/s/Mpc) used in object detection/filtering stage
    # Set-up the mass function stuff also
    # This is for correction of mass bias due to steep cluster mass function
    # Hence minMass here needs to be set well below the survey mass limit
    # areaDeg2 we don't care about here
    minMass=1e13
    areaDeg2=700.
    zMin=0.0
    zMax=2.0
    # H0, Om0, Ol0 used for E(z), theta500 calcs in Q - these are updated when we call create mockSurvey
    if 'H0' in list(massOptions.keys()):
        H0=massOptions['H0']
    else:
        H0=70.
    if 'Om0' in list(massOptions.keys()):
        Om0=massOptions['Om0']
    else:
        Om0=0.3
    # OmegaB0, sigma8 only used for mass function Eddington bias correction
    if 'Ob0' in list(massOptions.keys()):
        Ob0=massOptions['Ob0']
    else:
        Ob0=0.05
    if 'sigma_8' in list(massOptions.keys()):
        sigma_8=massOptions['sigma_8']
    else:
        sigma_8=0.8
    mockSurvey=MockSurvey.MockSurvey(minMass, areaDeg2, zMin, zMax, H0, Om0, Ob0, sigma_8)
    
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500_errPlus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500_errMinus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Uncorr"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Uncorr_errPlus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Uncorr_errMinus"))
    if 'rescaleFactor' in list(massOptions.keys()):
        tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Cal"))
        tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Cal_errPlus"))
        tab.add_column(atpy.Column(np.zeros(len(tab)), "M500Cal_errMinus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200m"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200m_errPlus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200m_errMinus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200mUncorr"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200mUncorr_errPlus"))
    tab.add_column(atpy.Column(np.zeros(len(tab)), "M200mUncorr_errMinus"))
    #tab.add_column(atpy.Column(np.zeros(len(tab)), "Q"))
    #tab.add_column(atpy.Column(np.zeros(len(tab)), "Q_err"))
    
    # For fRel calc (multi-freq)
    fRelKeysDict={}
    for key in tab.keys():
        if key.find("fixed_y_c_weight") != -1:
            obsFreqGHz=float(key.split("_")[-1].split("GHz")[0].replace("p", "."))  
            fRelKeysDict[key]=obsFreqGHz
    if fRelKeysDict == {}:
        print("... WARNING: no fixed_y_c_weight keys found in catalog - adding default ...")
        tab.add_column(atpy.Column(np.ones(len(tab)), 'fixed_y_c_weight_148p0GHz'))
        fRelKeysDict={'fixed_y_c_weight_148p0GHz': 148.0}
    
    count=0
    for row in tab:
        count=count+1
        extName=row['template'].split("#")[-1]
        fRelWeightsDict={}
        for key in fRelKeysDict:
            fRelWeightsDict[fRelKeysDict[key]]=row[key]
            
        print("... %d/%d %s (%.3f +/- %.3f) [fRelWeightsDict = %s] ..." % (count, len(tab), row['name'], row['redshift'], row['redshiftErr'], fRelWeightsDict))

        # Corrected for mass function steepness
        massDict=simsTools.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                          row['redshift'], row['redshiftErr'],
                                          tenToA0 = massOptions['tenToA0'],
                                          B0 = massOptions['B0'], 
                                          Mpivot = massOptions['Mpivot'], 
                                          sigma_int = massOptions['sigma_int'],
                                          tckQFit = tckQFitDict[extName], mockSurvey = mockSurvey, 
                                          applyMFDebiasCorrection = True,
                                          fRelWeightsDict = fRelWeightsDict)
        row['M500']=massDict['M500']
        row['M500_errPlus']=massDict['M500_errPlus']
        row['M500_errMinus']=massDict['M500_errMinus']
        # Uncorrected for mass function steepness
        unCorrMassDict=simsTools.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                                row['redshift'], row['redshiftErr'],
                                                tenToA0 = massOptions['tenToA0'],
                                                B0 = massOptions['B0'], 
                                                Mpivot = massOptions['Mpivot'], 
                                                sigma_int = massOptions['sigma_int'],
                                                tckQFit = tckQFitDict[extName], mockSurvey = mockSurvey, 
                                                applyMFDebiasCorrection = False,
                                                fRelWeightsDict = fRelWeightsDict)
        row['M500Uncorr']=unCorrMassDict['M500']
        row['M500Uncorr_errPlus']=unCorrMassDict['M500_errPlus']
        row['M500Uncorr_errMinus']=unCorrMassDict['M500_errMinus']
        # Mass conversion
        row['M200m']=simsTools.convertM500cToM200m(massDict['M500']*1e14, row['redshift'])/1e14
        row['M200m_errPlus']=(row['M500_errPlus']/row['M500'])*row['M200m']
        row['M200m_errMinus']=(row['M500_errMinus']/row['M500'])*row['M200m']
        row['M200mUncorr']=simsTools.convertM500cToM200m(unCorrMassDict['M500']*1e14, row['redshift'])/1e14
        row['M200mUncorr_errPlus']=(row['M500Uncorr_errPlus']/row['M500Uncorr'])*row['M200mUncorr']
        row['M200mUncorr_errMinus']=(row['M500Uncorr_errMinus']/row['M500Uncorr'])*row['M200mUncorr']
        # Re-scaling (e.g., using richness-based weak-lensing mass calibration)
        row['M500Cal']=massDict['M500']/massOptions['rescaleFactor']
        row['M500Cal_errPlus']=np.sqrt(np.power(row['M500_errPlus']/row['M500'], 2) + \
                                        np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500Cal']
        row['M500Cal_errMinus']=np.sqrt(np.power(row['M500_errMinus']/row['M500'], 2) + \
                                        np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500Cal']
        
    # Tidy up and save
    # We delete some columns here to save duplicating in sourcery database
    #tab.remove_columns(['fixed_SNR', 'fixed_y_c', 'fixed_err_y_c'])
    if os.path.exists(outFileName) == True:
        os.remove(outFileName)
    tab.write(outFileName)
    
    # Detect if testing a mock catalog, and write some stats on recovered masses
    if 'true_M500' in tab.keys():
        # Noise sources in mocks
        if 'applyPoissonScatter' in parDict.keys():
            applyPoissonScatter=parDict['applyPoissonScatter']
        else:
            applyPoissonScatter=True
        if 'applyIntrinsicScatter' in parDict.keys():
            applyIntrinsicScatter=parDict['applyIntrinsicScatter']
        else:
            applyIntrinsicScatter=True
        if 'applyNoiseScatter' in parDict.keys():
            applyNoiseScatter=parDict['applyNoiseScatter']
        else:
            applyNoiseScatter=True
        print(">>> Mock noise sources (Poisson, intrinsic, measurement noise) = (%s, %s, %s) ..." % (applyPoissonScatter, applyIntrinsicScatter, applyNoiseScatter))
        if applyNoiseScatter == True and applyIntrinsicScatter == True:
            print("... for these options, median M500 / true_M500 = 1.000 if mass recovery is unbiased ...")
        elif applyNoiseScatter == False and applyIntrinsicScatter == False:
            print("... for these options, median M500Unc / true_M500 = 1.000 if mass recovery is unbiased ...")
        else:
            print("... for these options, both median M500 / true_M500 and median M500Unc / true_M500 will be biased ...")
        ratio=tab['M500']/tab['true_M500']
        ratioUnc=tab['M500Uncorr']/tab['true_M500']
        print(">>> Mock catalog mass recovery stats:")
        SNRCuts=[4, 5, 7, 10]
        for s in SNRCuts:
            mask=np.greater(tab['fixed_SNR'], s)
            print("--> SNR > %.1f (N = %d):" % (s, np.sum(mask))) 
            print("... mean M500 / true_M500 = %.3f +/- %.3f (stdev) +/- %.3f (sterr)" % (np.mean(ratio[mask]), 
                                                                                          np.std(ratio[mask]), 
                                                                                          np.std(ratio[mask])/np.sqrt(np.sum(mask))))
            print("... median M500 / true_M500 = %.3f" % (np.median(ratio[mask])))
            print("... mean M500Unc / true_M500 = %.3f +/- %.3f (stdev) +/- %.3f (sterr)" % (np.mean(ratioUnc[mask]), 
                                                                                          np.std(ratioUnc[mask]), 
                                                                                          np.std(ratioUnc[mask])/np.sqrt(np.sum(mask))))
            print("... median M500Unc / true_M500 = %.3f" % (np.median(ratioUnc[mask])))



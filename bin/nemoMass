#!/usr/bin/env python

"""

Calculate masses of clusters detected by nemo

Requires 'massOptions' in nemo config file

Run this after filtering / detecting objects in a map with nemo itself

Can be used to obtain 'forced photometry', i.e., mass estimates for objects in redshiftCatalog
(for, e.g., optical stacking)

"""

import os
import sys
import numpy as np
import pylab as plt
import astropy.table as atpy
from astLib import *
from scipy import stats
from scipy import interpolate
from nemo import catalogs
from nemo import signals
from nemo import maps
from nemo import filters
from nemo import MockSurvey
from nemo import photometry
from nemo import startUp
from nemo import completeness
import argparse
import astropy.io.fits as pyfits
import time
import yaml
#import IPython

#------------------------------------------------------------------------------------------------------------
def addForcedPhotometry(forcedTab, config):
    """Given a tab with (minimum) name, RADeg, decDeg, redshift columns, add forced photometry columns so we
    can estimate SZ masses.
    
    """
    
    print(">>> Doing forced photometry ...")
    
    # Filter maps - but should already have been done, this is just for easy set up
    photFilterLabel=config.parDict['photFilter']
    photFilterList=[]
    for filterDict in config.parDict['mapFilters']:
        if filterDict['label'] == photFilterLabel:
            photFilterList.append(filterDict)
    imageDict=mapFilters.filterMaps(config.parDict['unfilteredMaps'], photFilterList, tileNames = config.tileNames, rootOutDir = config.rootOutDir)
    
    # Re-jig the catalog into the appropriate format before feeding through photometry routine
    catalog=[]
    for tileName in config.tileNames:
        # We need this to check if objects in the catalog are actually in the footprint
        areaMask, wcs=completeness.loadAreaMask(tileName, config.selFnDir)
        for row in forcedTab:
            objDict={}
            for key in list(forcedTab.keys()):
                objDict[key]=row[key]
            objDict['template']=photFilterLabel
            objDict['tileName']=tileName
            objDict['SNR']=1.0  # Just to keep photometry routine happy; not used for masses
            objDict['redshift']=row['redshift']
            if 'redshiftErr' in forcedTab.keys():
                objDict['redshiftErr']=row['redshiftErr']
            else:
                objDict['redshiftErr']=0.0
            x, y=wcs.wcs2pix(objDict['RADeg'], objDict['decDeg'])
            if x > 0 and y > 0 and x < areaMask.shape[1] and y < areaMask.shape[0]:
                if areaMask[int(round(y)), int(round(x))] == 1:
                    objDict['x']=x
                    objDict['y']=y
                    catalog.append(objDict)
    imageDict[photFilterLabel+"#"+tileName]['catalog']=catalog
    photometry.measureFluxes(imageDict, config.parDict['photFilter'], config.diagnosticsDir, unfilteredMapsDict = config.parDict['unfilteredMaps'])
    catalogs.mergeCatalogs(imageDict)
    
    # No cuts here, because if we're using this mode, the objects almost certainly weren't detected...
    catalogs.makeOptimalCatalog(imageDict, [])  
    photometry.addFreqWeightsToCatalog(imageDict, config.parDict['photFilter'], config.diagnosticsDir)

    # Make into an atpy Table
    # NOTE: we're going to extract the tile name from 'template'... not very elegant - should just write in the nemo catalog
    wantedKeys=['name', 'RADeg', 'decDeg', 'fixed_SNR', 'fixed_y_c', 'fixed_err_y_c', 'redshift', 'redshiftErr', 'template']
    for key in imageDict['optimalCatalog'][0].keys():
        if key.find("fixed_y_c_weight") != -1:
            wantedKeys.append(key)
    tab=atpy.Table()
    for tileName in config.tileNames:
        catalog=imageDict['optimalCatalog']
        for key in wantedKeys:
            arr=[]
            for objDict in catalog:
                if key in list(objDict.keys()):
                    arr.append(objDict[key])
                else:
                    arr.append(0)
            tab.add_column(atpy.Column(np.array(arr), key))
    tab=tab[np.where(tab['redshift'] != 0)]

    # Flag things with -ve y0~, which we can't do anything with (unless we add stacking / averaging mode)
    for row in tab:
        if row['fixed_y_c'] < 0:
            print("... %s has fixed_y_c < 0 (and is removed) ..." % (row['name']))
    tab=tab[np.where(tab['fixed_y_c'] > 0)]

    return tab

#------------------------------------------------------------------------------------------------------------
def calcMass(tab, massOptions, tckQFitDict, fRelWeightsDict):
    """Calculates masses for cluster data in table.
    """
    
    count=0
    for row in tab:
        count=count+1
        print("... rank %d; %d/%d; %s (%.3f +/- %.3f) ..." % (config.rank, count, len(tab), row['name'], 
                                                              row['redshift'], row['redshiftErr']))

        tileName=row['tileName']
        
        # Corrected for mass function steepness
        if row['fixed_y_c'] > 0:
            massDict=signals.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                            row['redshift'], row['redshiftErr'],
                                            tenToA0 = massOptions['tenToA0'],
                                            B0 = massOptions['B0'], 
                                            Mpivot = massOptions['Mpivot'], 
                                            sigma_int = massOptions['sigma_int'],
                                            tckQFit = tckQFitDict[tileName], mockSurvey = mockSurvey, 
                                            applyMFDebiasCorrection = True,
                                            applyRelativisticCorrection = True,
                                            fRelWeightsDict = fRelWeightsDict[tileName])
            row['M500']=massDict['M500']
            row['M500_errPlus']=massDict['M500_errPlus']
            row['M500_errMinus']=massDict['M500_errMinus']
            # Without relativistic correction (may be useful)
            massDict_noRel=signals.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                            row['redshift'], row['redshiftErr'],
                                            tenToA0 = massOptions['tenToA0'],
                                            B0 = massOptions['B0'], 
                                            Mpivot = massOptions['Mpivot'], 
                                            sigma_int = massOptions['sigma_int'],
                                            tckQFit = tckQFitDict[tileName], mockSurvey = mockSurvey, 
                                            applyMFDebiasCorrection = True,
                                            applyRelativisticCorrection = False,
                                            fRelWeightsDict = fRelWeightsDict[tileName])
            row['M500NoRel']=massDict_noRel['M500']
            row['M500NoRel_errPlus']=massDict_noRel['M500_errPlus']
            row['M500NoRel_errMinus']=massDict_noRel['M500_errMinus']
            # Uncorrected for mass function steepness
            unCorrMassDict=signals.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                                    row['redshift'], row['redshiftErr'],
                                                    tenToA0 = massOptions['tenToA0'],
                                                    B0 = massOptions['B0'], 
                                                    Mpivot = massOptions['Mpivot'], 
                                                    sigma_int = massOptions['sigma_int'],
                                                    tckQFit = tckQFitDict[tileName], mockSurvey = mockSurvey, 
                                                    applyMFDebiasCorrection = False,
                                                    applyRelativisticCorrection = True,
                                                    fRelWeightsDict = fRelWeightsDict)
            row['M500Uncorr']=unCorrMassDict['M500']
            row['M500Uncorr_errPlus']=unCorrMassDict['M500_errPlus']
            row['M500Uncorr_errMinus']=unCorrMassDict['M500_errMinus']
            # Uncorrected for mass function steepness - without relativistic correction
            unCorrMassDict_noRel=signals.calcM500Fromy0(row['fixed_y_c']*1e-4, row['fixed_err_y_c']*1e-4, 
                                                    row['redshift'], row['redshiftErr'],
                                                    tenToA0 = massOptions['tenToA0'],
                                                    B0 = massOptions['B0'], 
                                                    Mpivot = massOptions['Mpivot'], 
                                                    sigma_int = massOptions['sigma_int'],
                                                    tckQFit = tckQFitDict[tileName], mockSurvey = mockSurvey, 
                                                    applyMFDebiasCorrection = False,
                                                    applyRelativisticCorrection = False,
                                                    fRelWeightsDict = fRelWeightsDict)
            row['M500UncorrNoRel']=unCorrMassDict_noRel['M500']
            row['M500UncorrNoRel_errPlus']=unCorrMassDict_noRel['M500_errPlus']
            row['M500UncorrNoRel_errMinus']=unCorrMassDict_noRel['M500_errMinus']
            # Mass conversion
            row['M200m']=signals.convertM500cToM200m(massDict['M500']*1e14, row['redshift'])/1e14
            row['M200m_errPlus']=(row['M500_errPlus']/row['M500'])*row['M200m']
            row['M200m_errMinus']=(row['M500_errMinus']/row['M500'])*row['M200m']
            row['M200mUncorr']=signals.convertM500cToM200m(unCorrMassDict['M500']*1e14, row['redshift'])/1e14
            row['M200mUncorr_errPlus']=(row['M500Uncorr_errPlus']/row['M500Uncorr'])*row['M200mUncorr']
            row['M200mUncorr_errMinus']=(row['M500Uncorr_errMinus']/row['M500Uncorr'])*row['M200mUncorr']
            # Mass conversion - no relativistic correction
            row['M200mNoRel']=signals.convertM500cToM200m(massDict_noRel['M500']*1e14, row['redshift'])/1e14
            row['M200mNoRel_errPlus']=(row['M500NoRel_errPlus']/row['M500NoRel'])*row['M200mNoRel']
            row['M200mNoRel_errMinus']=(row['M500NoRel_errMinus']/row['M500NoRel'])*row['M200mNoRel']
            row['M200mUncorrNoRel']=signals.convertM500cToM200m(unCorrMassDict_noRel['M500']*1e14, row['redshift'])/1e14
            row['M200mUncorrNoRel_errPlus']=(row['M500UncorrNoRel_errPlus']/row['M500UncorrNoRel'])*row['M200mUncorrNoRel']
            row['M200mUncorrNoRel_errMinus']=(row['M500UncorrNoRel_errMinus']/row['M500UncorrNoRel'])*row['M200mUncorrNoRel']
            # Re-scaling (e.g., using richness-based weak-lensing mass calibration)
            row['M500Cal']=massDict['M500']/massOptions['rescaleFactor']        
            row['M500Cal_errPlus']=np.sqrt(np.power(row['M500_errPlus']/row['M500'], 2) + \
                                            np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500Cal']
            row['M500Cal_errMinus']=np.sqrt(np.power(row['M500_errMinus']/row['M500'], 2) + \
                                            np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500Cal']
            row['M200mCal']=signals.convertM500cToM200m(row['M500Cal']*1e14, row['redshift'])/1e14
            row['M200mCal_errPlus']=(row['M500Cal_errPlus']/row['M500Cal'])*row['M200mCal']
            row['M200mCal_errMinus']=(row['M500Cal_errMinus']/row['M500Cal'])*row['M200mCal']
            # Re-scaling (e.g., using richness-based weak-lensing mass calibration) - no relativistic correction
            row['M500CalNoRel']=massDict_noRel['M500']/massOptions['rescaleFactor']        
            row['M500CalNoRel_errPlus']=np.sqrt(np.power(row['M500NoRel_errPlus']/row['M500NoRel'], 2) + \
                                            np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500CalNoRel']
            row['M500CalNoRel_errMinus']=np.sqrt(np.power(row['M500NoRel_errMinus']/row['M500NoRel'], 2) + \
                                            np.power(massOptions['rescaleFactorErr']/massOptions['rescaleFactor'], 2))*row['M500CalNoRel']
            row['M200mCalNoRel']=signals.convertM500cToM200m(row['M500CalNoRel']*1e14, row['redshift'])/1e14
            row['M200mCalNoRel_errPlus']=(row['M500CalNoRel_errPlus']/row['M500CalNoRel'])*row['M200mCalNoRel']
            row['M200mCalNoRel_errMinus']=(row['M500CalNoRel_errMinus']/row['M500CalNoRel'])*row['M200mCalNoRel']

    return tab

#------------------------------------------------------------------------------------------------------------
if __name__ == '__main__':

    parser=argparse.ArgumentParser("nemoMass")
    parser.add_argument("configFileName", help="""A .yml configuration file.""")
    parser.add_argument("-c", "--catalog", dest="catFileName", help = """Catalog file name (.fits format).
                        The catalog must contain at least the following columns: name, RADeg, decDeg, 
                        redshift, redshiftErr. If the catalog contains fixed_y_c, fixed_err_y_c columns, 
                        then these will be used to infer mass estimates. If not, "forced photometry" mode 
                        will be enabled, and the fixed_y_c, fixed_err_y_c values will be extracted from the
                        filtered maps.""", default = None)
    parser.add_argument("-M", "--mpi", dest="MPIEnabled", action="store_true", help="""Enable MPI. If you 
                        want to use this, run with e.g., mpiexec -np 4 nemoMass configFile.yml -M""", 
                        default = False)
    args = parser.parse_args()
    
    parDictFileName=args.configFileName
    catFileName=args.catFileName
    config=startUp.NemoConfig(parDictFileName, MPIEnabled = args.MPIEnabled, setUpMaps = False, verbose = False)
        
    massOptions=config.parDict['massOptions']
    if 'relativisticCorrection' not in massOptions.keys():
        massOptions['relativisticCorrection']=True
        
    forcedPhotometry=False
    
    # Load the nemo catalog and match against the z catalog
    if catFileName == None:
        optimalCatalogFileName=config.rootOutDir+os.path.sep+"%s_optimalCatalog.fits" % (os.path.split(config.rootOutDir)[-1])           
        nemoTab=atpy.Table().read(optimalCatalogFileName)
        zTab=atpy.Table().read(massOptions['redshiftCatalog'])
        # Sanity check/clean z table
        # We're matching on name alone before, so keep only one row with unique name
        wantedColumns=['name', 'RADeg', 'decDeg', 'redshift', 'redshiftErr']
        colsToRemove=[]
        for key in zTab.keys():
            if key not in wantedColumns:
                colsToRemove.append(key)
        zTab.remove_columns(colsToRemove)
        keepIndices=[]
        gotClusters=[]
        for row in zTab:
            matched=np.where(row['name'] == zTab['name'])
            if row['name'] not in gotClusters:
                if len(np.unique(zTab[matched]['redshift'])) != 1 and config.rank == 0:
                    print("... WARNING: Found redshifts %s for cluster %s - adopted z = %.3f" 
                        % (str(zTab['redshift'][matched].tolist()), row['name'], row['redshift']))
                keepIndices.append(matched[0][0])
                gotClusters.append(row['name'])
        zTab=zTab[keepIndices]
        assert(len(np.unique(zTab['name'])) == len(zTab))
        # Now join...
        tab=atpy.join(nemoTab, zTab, 'name')
        colsToRemove=[]
        for k in list(tab.keys()):
            if k[-2:] == '_2':
                colsToRemove.append(k)
        tab.remove_columns(colsToRemove)
        # NOTE: we're going to extract the tile name from 'template'... not very elegant - should just write in the nemo catalog
        colsToKeep=['name', 'RADeg', 'decDeg', 'fixed_SNR', 'fixed_y_c', 'fixed_err_y_c', 'redshift', 'redshiftErr', 'template', 'tileName']
        for k in colsToKeep:
            if k+"_1" in list(tab.keys()):
                tab.rename_column(k+"_1", k)
        colsToRemove=[]
        for k in list(tab.keys()):
            if k not in colsToKeep:
                colsToRemove.append(k)
        tab.remove_columns(colsToRemove)
        outFileName=optimalCatalogFileName.replace("_optimalCatalog.fits", "_M500.fits")

    else:
        
        # Load another catalog (e.g., a mock, for testing)
        optimalCatalogFileName=catFileName
        tab=atpy.Table().read(optimalCatalogFileName)
        outFileName=catFileName.replace(".fits", "_M500.fits")
        
        # Enter forced photometry mode if we can't find the columns we need
        keysNeeded=['fixed_y_c', 'fixed_err_y_c']
        forcedPhotometry=False
        for key in keysNeeded:
            if key not in tab.keys():
                forcedPhotometry=True

    # Q function (filter mismatch) options
    tckQFitDict=signals.loadQ(config)
    
    # Dictionary by tile with frequency weights needed by relativistic corrections
    fRelWeightsDict=signals.getFRelWeights(config)
    
    # Forced photometry (if enabled) - modifying table in place
    # NOTE: Move this up if/when we make it run under MPI
    if forcedPhotometry == True:
        tab=addForcedPhotometry(tab, config)

    # Optional fixed SNR cut
    if 'fixedSNRCut' in list(massOptions.keys()):
        tab=tab[np.where(tab['fixed_SNR'] > massOptions['fixedSNRCut'])]
    
    # Set cosmological parameters for e.g. E(z) calc if these are set in .par file
    # We set them after the Q calc, because the Q calc needs to be for the fiducial cosmology
    # (OmegaM0 = 0.3, OmegaL0 = 0.7, H0 = 70 km/s/Mpc) used in object detection/filtering stage
    # Set-up the mass function stuff also
    # This is for correction of mass bias due to steep cluster mass function
    # Hence minMass here needs to be set well below the survey mass limit
    # areaDeg2 we don't care about here
    minMass=1e13
    areaDeg2=700.
    zMin=0.0
    zMax=2.0
    # H0, Om0, Ol0 used for E(z), theta500 calcs in Q - these are updated when we call create mockSurvey
    if 'H0' in list(massOptions.keys()):
        H0=massOptions['H0']
    else:
        H0=70.
    if 'Om0' in list(massOptions.keys()):
        Om0=massOptions['Om0']
    else:
        Om0=0.3
    # OmegaB0, sigma8 only used for mass function Eddington bias correction
    if 'Ob0' in list(massOptions.keys()):
        Ob0=massOptions['Ob0']
    else:
        Ob0=0.05
    if 'sigma_8' in list(massOptions.keys()):
        sigma_8=massOptions['sigma_8']
    else:
        sigma_8=0.8
    mockSurvey=MockSurvey.MockSurvey(minMass, areaDeg2, zMin, zMax, H0, Om0, Ob0, sigma_8)
    
    colsToAdd=['M500', 'M500Uncorr', 'M200m', 'M200mUncorr',
               'M500NoRel', 'M500UncorrNoRel', 'M200mNoRel', 'M200mUncorrNoRel']
    if 'rescaleFactor' in list(massOptions.keys()):
        colsToAdd=colsToAdd+['M500Cal', 'M200mCal', 'M500CalNoRel', 'M200mCalNoRel']
    for c in colsToAdd:
        tab.add_column(atpy.Column(np.zeros(len(tab)), c))
        tab.add_column(atpy.Column(np.zeros(len(tab)), c+"_errPlus"))
        tab.add_column(atpy.Column(np.zeros(len(tab)), c+"_errMinus"))
    
    #if massOptions['relativisticCorrection'] == False:
        #relStr="no relativistic correction applied"
    #else:
        #relStr="relativistic correction applied"
    #if config.rank == 0: print(">>> Estimating masses (%s) ..." % (relStr))

    # Seems like multiprocessing doesn't work under slurm...   
    # So use MPI instead: just divide up catalog among processes
    tab.add_column(atpy.Column(np.arange(len(tab)), "sortIndex"))
    if config.MPIEnabled == True:
        numRowsPerProcess=int(np.ceil(len(tab)/config.size))
        startIndex=config.rank*numRowsPerProcess
        endIndex=startIndex+numRowsPerProcess
        if config.rank == config.size-1:
            endIndex=len(tab)
        tab=tab[startIndex:endIndex]
        
    tab=calcMass(tab, massOptions, tckQFitDict, fRelWeightsDict)
    
    if config.MPIEnabled == True:
        tabList=config.comm.gather(tab, root = 0)
        if config.rank != 0:
            assert tabList is None
            print("... MPI rank %d finished ..." % (config.rank))
            sys.exit()
        else:
            print("... gathering catalogs ...")
            tab=atpy.vstack(tabList)
    
    tab.sort('sortIndex')
    tab.remove_column('sortIndex')               
    
    # Tidy up and save
    # We delete some columns here to save duplicating in sourcery database
    #tab.remove_columns(['fixed_SNR', 'fixed_y_c', 'fixed_err_y_c'])
    tab.write(outFileName, overwrite = True)
    
    # Detect if testing a mock catalog, and write some stats on recovered masses
    if 'true_M500' in tab.keys():
        # Noise sources in mocks
        if 'applyPoissonScatter' in config.parDict.keys():
            applyPoissonScatter=config.parDict['applyPoissonScatter']
        else:
            applyPoissonScatter=True
        if 'applyIntrinsicScatter' in config.parDict.keys():
            applyIntrinsicScatter=config.parDict['applyIntrinsicScatter']
        else:
            applyIntrinsicScatter=True
        if 'applyNoiseScatter' in config.parDict.keys():
            applyNoiseScatter=config.parDict['applyNoiseScatter']
        else:
            applyNoiseScatter=True
        print(">>> Mock noise sources (Poisson, intrinsic, measurement noise) = (%s, %s, %s) ..." % (applyPoissonScatter, applyIntrinsicScatter, applyNoiseScatter))
        if applyNoiseScatter == True and applyIntrinsicScatter == True:
            print("... for these options, median M500 / true_M500 = 1.000 if mass recovery is unbiased ...")
        elif applyNoiseScatter == False and applyIntrinsicScatter == False:
            print("... for these options, median M500Unc / true_M500 = 1.000 if mass recovery is unbiased ...")
        else:
            print("... for these options, both median M500 / true_M500 and median M500Unc / true_M500 will be biased ...")
        ratio=tab['M500']/tab['true_M500']
        ratioUnc=tab['M500Uncorr']/tab['true_M500']
        print(">>> Mock catalog mass recovery stats:")
        SNRCuts=[4, 5, 7, 10]
        for s in SNRCuts:
            mask=np.greater(tab['fixed_SNR'], s)
            print("--> SNR > %.1f (N = %d):" % (s, np.sum(mask))) 
            print("... mean M500 / true_M500 = %.3f +/- %.3f (stdev) +/- %.3f (sterr)" % (np.mean(ratio[mask]), 
                                                                                          np.std(ratio[mask]), 
                                                                                          np.std(ratio[mask])/np.sqrt(np.sum(mask))))
            print("... median M500 / true_M500 = %.3f" % (np.median(ratio[mask])))
            print("... mean M500Unc / true_M500 = %.3f +/- %.3f (stdev) +/- %.3f (sterr)" % (np.mean(ratioUnc[mask]), 
                                                                                          np.std(ratioUnc[mask]), 
                                                                                          np.std(ratioUnc[mask])/np.sqrt(np.sum(mask))))
            print("... median M500Unc / true_M500 = %.3f" % (np.median(ratioUnc[mask])))



#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""

Calculate mass / y completeness limits over the fixed scale filtered map

"""

import os
import sys
import resource
import glob
import numpy as np
import pylab as plt
import astropy.table as atpy
from astLib import *
from scipy import stats
from scipy import interpolate
from scipy import ndimage
from nemo import actDict
from nemo import simsTools
from nemo import mapTools
from nemo import MockSurvey
from nemo import SelFn
from nemo import photometry
from nemo import plotSettings
import pickle
import nemoCython
import pyfits
import time
import matplotlib.cm as cm
import matplotlib
import IPython
plt.matplotlib.interactive(False)

cm.register_cmap("planck", matplotlib.colors.LinearSegmentedColormap.from_list("planck",[(0,"#0000ff"),(0.332,"#00d7ff"),(0.5,"#ffedd9"),(0.664,"#ffb400"),(0.828,"#ff4b00"),(1,"#640000")]))

#------------------------------------------------------------------------------------------------------------
def makeSimTab(numRows):
    """Returns a blank table object with given number of rows. Used to store selection function output.
    
    """
    
    simTab=atpy.Table()
    simTab.add_column(atpy.Column(np.zeros(numRows), 'ID'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'y0'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'y1'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'x0'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'x1'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'z'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'M500MSun'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'yc'))
    simTab.add_column(atpy.Column(np.zeros(numRows), 'SNR'))
    
    return simTab

#------------------------------------------------------------------------------------------------------------
def generatePositions(numPositions, surveyMask, x0, x1, y0, y1, maxTries = 10):
    """Returns random x, y positions in surveyMask between the given pixel limits.
    
    """
                    
    possiblePositions=np.where(surveyMask[y0:y1, x0:x1] == 1)
    xPositions, yPositions=[], []
    for n in xrange(numPositions):
        maskSum=0
        tryCount=0
        while maskSum == 0:
            randIndex=np.random.randint(len(possiblePositions[0]))
            yPosition, xPosition=possiblePositions[0][randIndex]+y0, possiblePositions[1][randIndex]+x0
            maskSum=surveyMask[yPosition-3:yPosition+3, xPosition-3:xPosition+3].sum()
            if surveyMask[yPosition, xPosition] == 0:
                maskSum=0
            if tryCount > maxTries:
                maskSum=1
            tryCount=tryCount+1
        if tryCount < maxTries:
            xPositions.append(xPosition)
            yPositions.append(yPosition)
    
    return xPositions, yPositions

#------------------------------------------------------------------------------------------------------------
def doSquare(mapData, RMSMap, surveyMask, wcs, numPositions, squareID, x0, x1, y0, y1, clipSizeDeg, zRange, MRange, 
             signalDictsByRedshift, kern2d, bckScaleDeg, sigNorm, minNotMaskedPix = 0):
    """Returns simTab containing results for this square only. If masked or empty, returns []
    
    """

    # Quickly skip near-empty squares
    if surveyMask[y0:y1, x0:x1].sum() <= minNotMaskedPix:
        return []

    # Generate a bunch of random positions - remember default squares are 20' on a side
    # Skip on if all positions masked
    xPositions, yPositions=generatePositions(numPositions, surveyMask, x0, x1, y0, y1)
    if len(xPositions) == 0:
        return []
    
    # Re-jigged to avoid gobbling all the RAM
    SNRResults={}
    ycResults={}
    for z in zRange:
        if z not in SNRResults.keys():
            SNRResults[z]={}
        if z not in ycResults.keys():
            ycResults[z]={}
        for M in MRange:
            if M not in SNRResults.keys():
                SNRResults[z][M]=[]
            if M not in ycResults.keys():
                ycResults[z][M]=[]
    
    for xPosition, yPosition in zip(xPositions, yPositions):
        RADeg, decDeg=wcs.pix2wcs(xPosition, yPosition)
        clip=astImages.clipImageSectionWCS(mapData, wcs, RADeg, decDeg, clipSizeDeg)
        RMSClip=astImages.clipImageSectionWCS(RMSMap, wcs, RADeg, decDeg, clipSizeDeg)
        yCentre=clip['data'].shape[0]/2
        xCentre=clip['data'].shape[1]/2
        clipData=np.zeros(clip['data'].shape)+clip['data']  
        for z in zRange:
            for signalDict in signalDictsByRedshift['%.2f' % (z)]:
                
                M=signalDict['M500MSun']
                
                # Splat in cluster
                sy0=yCentre-signalDict['signalMap'].shape[0]/2
                sy1=yCentre+signalDict['signalMap'].shape[0]/2
                sx0=xCentre-signalDict['signalMap'].shape[1]/2
                sx1=xCentre+signalDict['signalMap'].shape[1]/2
                
                try:
                    filteredMap=clipData[sy0:sy1, sx0:sx1]+signalDict['signalMap']
                except:
                    # Edge of map
                    break
                
                # Apply kernel - convolution takes most of the time (0.02 sec)
                filteredMap=mapTools.subtractBackground(filteredMap, wcs, smoothScaleDeg = bckScaleDeg)
                filteredMap=ndimage.convolve(filteredMap, kern2d, mode = 'constant')*sigNorm 
                
                # Make SNMap
                SNMap=filteredMap/RMSClip['data'][sy0:sy1, sx0:sx1]
                syc=SNMap.shape[0]/2
                sxc=SNMap.shape[1]/2
                peakCoords=np.where(SNMap[syc-1:syc+1, sxc-1:sxc+1] == \
                                    SNMap[syc-1:syc+1, sxc-1:sxc+1].max())
                SNR=SNMap[syc-1:syc+1, sxc-1:sxc+1][peakCoords][0]
                yc=filteredMap[syc-1:syc+1, sxc-1:sxc+1][peakCoords][0]
                
                SNRResults[z][M].append(SNR)
                ycResults[z][M].append(yc)

    # Average and store
    numRows=len(zRange)*len(MRange)
    simTab=makeSimTab(numRows)
    rowCount=0
    for z in zRange:
        for M in MRange:
            medSNR=np.median(SNRResults[z][M])
            medyc=np.median(ycResults[z][M])
            if np.isnan(medSNR) == False and medyc > 0:
                simTab['ID'][rowCount]=squareID
                simTab['y0'][rowCount]=y0
                simTab['y1'][rowCount]=y1
                simTab['x0'][rowCount]=x0
                simTab['x1'][rowCount]=x1
                simTab['z'][rowCount]=z
                # NOTE: Below this is a label for the template shape, NOT a 'real' mass - need actual yc-M relation for that (see SelFn.py)
                simTab['M500MSun'][rowCount]=M
                simTab['yc'][rowCount]=medyc
                simTab['SNR'][rowCount]=medSNR
            rowCount=rowCount+1
    
    # Throw out empty rows
    simTab=simTab[np.where(simTab['ID'] != 0)]

    return simTab
                    
#------------------------------------------------------------------------------------------------------------
# Main
if len(sys.argv) < 2:
    print "Run: % nemoSelFn < .par file(s)>"
else:
    
    parDictFileNames=sys.argv[1:]
    
    for parDictFileName in parDictFileNames:
    
        print ">>> Running .par file: %s" % (parDictFileName)
        parDict=actDict.ACTDict()
        parDict.read_from_file(parDictFileName)

        MPIEnabled=parDict['useMPI']
        
        if MPIEnabled ==True:
            from mpi4py import MPI
            comm=MPI.COMM_WORLD
            size=comm.Get_size()
            rank=comm.Get_rank()
            if size == 1:
                raise Exception, "if you want to use MPI, run with e.g., mpirun --np 4 nemoSelFn ..."
        else:
            rank=0

        # Output dirs
        if 'outputDir' in parDict.keys():
            rootOutDir=parDict['outDir']
        else:
            if parDictFileName.find(".par") == -1:
                raise Exception, "File must have .par extension"
            rootOutDir=sys.argv[1].replace(".par", "")
        filteredMapsDir=rootOutDir+os.path.sep+"filteredMaps"
        filtersDir=rootOutDir+os.path.sep+"filters"
        diagnosticsDir=rootOutDir+os.path.sep+"diagnostics"
        dirList=[rootOutDir, filteredMapsDir, filtersDir]
        for d in dirList:
            if os.path.exists(d) == False and rank == 0:
                os.makedirs(d)

        # Filter maps - but should already have been done, this is just for easy set up
        imageDict=mapTools.filterMaps(parDict['unfilteredMaps'], parDict['mapFilters'], rootOutDir = rootOutDir)
        
        # We only care about the filter used for fixed_ columns
        photFilterLabel=parDict['photometryOptions']['photFilter']
        for filterDict in parDict['mapFilters']:
            if filterDict['label'] == photFilterLabel:
                break

        # Focus ONLY on RealSpaceMatchedFilter here... AND assume single-frequency only (kernels saved are anyway)
        # Convert to y and invert
        mapDict=imageDict[photFilterLabel]['unfilteredMapsDictList'][0]
        img=pyfits.open(mapDict['mapFileName'])
        wcs=astWCS.WCS(img[0].header, mode = 'pyfits')
        mapData=img[0].data*-1
        mapData=mapTools.convertToY(mapData, mapDict['obsFreqGHz'])
        beamFileName=mapDict['beamFileName']

        # Load the kernel
        kernImg=pyfits.open(diagnosticsDir+os.path.sep+"kern2d_%s.fits" % (photFilterLabel))
        kern2d=kernImg[0].data
        bckScaleDeg=kernImg[0].header['BCKSCALE']/60.
        sigNorm=kernImg[0].header['SIGNORM']
    
        # Survey mask
        surveyImg=pyfits.open(diagnosticsDir+os.path.sep+"areaMask.fits")
        surveyMask=surveyImg[0].data
        #mapShape=[wcs.header['NAXIS2'], wcs.header['NAXIS1']]

        # Unmasked area map, square degrees
        areaMapSqDeg=(mapTools.getPixelAreaArcmin2Map(surveyMask, wcs)*surveyMask)/(60**2)
        print "... total unmasked survey area = %.3f square degrees ..." % (areaMapSqDeg.sum())
        
        # Noise map
        RMSImg=pyfits.open(diagnosticsDir+os.path.sep+"RMSMap_%s.fits" % (photFilterLabel))
        RMSMap=RMSImg[0].data
        
        # Write big sim table - this takes ~9 hours to do on a single core for equD56
        simTabFileName=diagnosticsDir+os.path.sep+"simTab.fits"
        zRange=SelFn.zRange
        MRange=SelFn.MRange
        
        # Only making the big sim table is parallelised
        if os.path.exists(simTabFileName) == True:
            if MPIEnabled == True and rank != 0:       
                sys.exit()
            print "... reading saved sim table ..."
            simTab=atpy.Table().read(simTabFileName)
            
        else:
            
            # Compute all the signal templates we want to use... cache for later
            pickleFileName=diagnosticsDir+os.path.sep+"signalDicts.pickled"
            if os.path.exists(pickleFileName) == False:
                t0=time.time()
                signalDictsByRedshift={}
                for z in zRange:
                    signalDictList=[]
                    for M500MSun in MRange:
                    
                        # Some faffing to get a smaller map we can splat model clusters into
                        RADeg, decDeg=wcs.getCentreWCSCoords()
                        clipDict=astImages.clipImageSectionWCS(mapData, wcs, RADeg, decDeg, 1.0)
                        signalWCS=clipDict['wcs']
                        signalShape=list(clipDict['data'].shape)
                        for ax in [0, 1]:
                            if signalShape[ax] % 2 != 0:
                                signalShape[ax]=signalShape[ax]+1            
                        signalMap=np.zeros(signalShape)
                        degreesMap=nemoCython.makeDegreesDistanceMap(signalMap, signalWCS, RADeg, decDeg, 1.0)
                    
                        # Model we'll be inserting - we turn it into yc, so obsFreqGHz shouldn't matter here
                        signalMap, modelDict=simsTools.makeArnaudModelSignalMap(z, M500MSun, mapDict['obsFreqGHz'], 
                                                                                degreesMap, signalWCS, beamFileName)
                        signalMap=mapTools.convertToY(signalMap.data, obsFrequencyGHz = mapDict['obsFreqGHz'])
                        signalDictList.append({'signalMap': signalMap, 'sizeDeg': signalWCS.getFullSizeSkyDeg(), 
                                               'modelDict': modelDict, 'z': z, 'M500MSun': M500MSun})
                    signalDictsByRedshift['%.2f' % (z)]=signalDictList
                t1=time.time()
                pickleFile=file(pickleFileName, "wb")
                pickler=pickle.Pickler(pickleFile)
                pickler.dump(signalDictsByRedshift)
                pickleFile.close()
                print "... making models took %.3f sec ..." % (t1-t0)
            else:
                print "... loading pickled signal maps ..."
                pickleFile=file(pickleFileName, "rb")
                unpickler=pickle.Unpickler(pickleFile)
                signalDictsByRedshift=unpickler.load()
                pickleFile.close()
                
            clipSizeDeg=float(np.max(signalDictsByRedshift[signalDictsByRedshift.keys()[0]][0]['sizeDeg'])*1.1)
                    
            # Do each grid square in noise map in turn
            # Avoids overlapping sources, and easy to parallelise later
            # Here we'll just store the raw results as a big table - we'll do the solving for SNR limits later
            # This will take ~9 hours to run on a single core
            print ">>> Measuring completeness across map ..."
            numPositions=parDict['selFnOptions']['maxPositionsPerSquare']
            gridSizeArcmin=filterDict['params']['noiseParams']['noiseGridArcmin']
            gridSize=int(round((gridSizeArcmin/60.)/wcs.getPixelSizeDeg()))
            overlapPix=0#gridSize/2 # Match the noise map
            numXChunks=mapData.shape[1]/gridSize
            numYChunks=mapData.shape[0]/gridSize
            yChunks=np.linspace(0, mapData.shape[0], numYChunks+1, dtype = int)
            xChunks=np.linspace(0, mapData.shape[1], numXChunks+1, dtype = int)
            
            # We can use this to skip near-empty squares
            minNotMaskedPix=0#int(round(0.25*gridSize*gridSize))
                        
            # Break down by number of squares rather than rows/columns
            #numSquares=400
            numSquares=(len(xChunks)-1)*(len(yChunks)-1)
            if MPIEnabled == True:
                numSquaresPerNode=numSquares/size
                startIndex=numSquaresPerNode*rank
                if rank == size-1:
                    endIndex=numSquares
                else:
                    endIndex=numSquaresPerNode*(rank+1)
            else:
                startIndex=0
                endIndex=numSquares
                        
            # The easiest way to keep track of where everything is
            squareIDs=[]
            kList=[]
            iList=[]
            for k in xrange(len(xChunks)-1):
                for i in xrange(len(yChunks)-1):
                    squareIDs.append(i*(len(xChunks)-1)+k)
                    kList.append(k)
                    iList.append(i)
            
            # A map of where the squares are - both eases debugging AND makes finding intersections with other masks fast
            squaresMapPath=diagnosticsDir+os.path.sep+"squaresMap.fits"
            if rank == 0 and os.path.exists(squaresMapPath) == False:
                squaresMap=np.zeros(mapData.shape)
                for squareID, i, k in zip(squareIDs, iList, kList):
                    y0=yChunks[i]-overlapPix
                    y1=yChunks[i+1]+overlapPix
                    x0=xChunks[k]-overlapPix
                    x1=xChunks[k+1]+overlapPix
                    if y0 < 0:
                        y0=0
                    if y1 > mapData.shape[0]:
                        y1=mapData.shape[0]
                    if x0 < 0:
                        x0=0
                    if x1 > mapData.shape[1]:
                        x1=mapData.shape[1]
                    squaresMap[y0:y1, x0:x1]=squareID
                astImages.saveFITS(squaresMapPath, squaresMap*surveyMask, wcs)
             
            # Uncomment if want to select a specific square for testing
            #wantedSquareID=5785
            #startIndex=squareIDs.index(wantedSquareID)
            #endIndex=startIndex+5
            
            # Write one simTab_*.fits file per processor (if MPI), and merge afterwards
            nodeFileName=diagnosticsDir+os.path.sep+"simTab_%d.fits" % (rank)
            if os.path.exists(nodeFileName) == False:
                count=0
                simTab=makeSimTab(0)
                for squareID, i, k in zip(squareIDs[startIndex:endIndex], iList[startIndex:endIndex], kList[startIndex:endIndex]):
                    
                    y0=yChunks[i]-overlapPix
                    y1=yChunks[i+1]+overlapPix
                    x0=xChunks[k]-overlapPix
                    x1=xChunks[k+1]+overlapPix
                    if y0 < 0:
                        y0=0
                    if y1 > mapData.shape[0]:
                        y1=mapData.shape[0]
                    if x0 < 0:
                        x0=0
                    if x1 > mapData.shape[1]:
                        x1=mapData.shape[1]
                        
                    t0=time.time()
                    squareTab=doSquare(mapData, RMSMap, surveyMask, wcs, numPositions, squareID, x0, x1, y0, y1, clipSizeDeg, zRange, MRange, 
                                       signalDictsByRedshift, kern2d, bckScaleDeg, sigNorm, minNotMaskedPix = 0)
                    if len(squareTab) > 0:
                        simTab=atpy.operations.vstack([simTab, squareTab])
                    t1=time.time()
                    count=count+1
                    
                    print "... rank %d / squareID = %d (%d/%d) / RAM = %.3f Gb / time taken = %.3f sec ..." % (rank, squareID, count, endIndex-startIndex,
                                                                                          resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1e6, t1-t0)
                    
                # Save this node's work (bad if running in serial... but suspect we won't want to)
                simTab.write(nodeFileName)
            else:
                print "... already made %s ..." % (nodeFileName)

            # Combine all the tables for each yChunk row
            # We can't just send/receive without changing stuff above (we'd need to make rank 0 just collect stuff)
            if rank == 0:
                simTab=atpy.Table().read(diagnosticsDir+os.path.sep+"simTab_0.fits")
                if MPIEnabled == True:
                    print ">>> Gathering tables from each node:"
                    for k in range(1, size):
                        print "... %d/%d ..." % (k+1, size)
                        nodeFileName=diagnosticsDir+os.path.sep+"simTab_%d.fits" % (k)
                        loadedTab=False
                        while loadedTab == False:
                            if os.path.exists(nodeFileName) == True:
                                nodeTab=atpy.Table().read(nodeFileName)
                                loadedTab=True
                            else:
                                print "... not found %s - waiting 60 sec ..." % (nodeFileName)
                                time.sleep(60)
                        mask=np.greater(nodeTab['z'], 0)
                        if mask.sum() > 0:
                            nodeTab=nodeTab[np.where(mask)]
                            simTab=atpy.operations.vstack([simTab, nodeTab])
                simTab.write(simTabFileName)
                print "... saved sim table %s ..." % (simTabFileName)
                ## Clean up the chunk files
                #fileList=glob.glob(diagnosticsDir+os.path.sep+"simTab_*.fits")
                #for f in fileList:
                    #os.remove(f)
            else:
                sys.exit()
                                                                
        # Now fit for M500 / y0 limits in each grid square
        # NOTE: Units change here (10^14 MSun, 1e-4 yc)
        # Linear regression seems to work okay
        # This is slow (takes ~1 hour) and could be parallelised
        outFileName=diagnosticsDir+os.path.sep+"fitSimTab.fits"
        if os.path.exists(outFileName) == False:
            print ">>> Fitting for M, yc as function of SNR ..."
            t0=time.time()
            squareIDs=np.unique(simTab['ID'])
            fitTab=atpy.Table()
            lenFitTab=len(squareIDs)*len(zRange)
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'ID'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'z'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'y0'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'y1'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'x0'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'x1'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'areaDeg2'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'fracSurveyArea'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'MFitSlope'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'MFitIntercept'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'ycFitSlope'))
            fitTab.add_column(atpy.Column(np.zeros(lenFitTab), 'ycFitIntercept'))
            rowCount=0
            for sID in squareIDs:
                for z in zRange:
                    mask=np.logical_and(np.equal(simTab['ID'], sID), np.equal(simTab['z'], z))
                    MFitResult=stats.linregress(simTab['SNR'][mask], simTab['M500MSun'][mask]/1e14)
                    ycFitResult=stats.linregress(simTab['SNR'][mask], simTab['yc'][mask])
                    fitTab['ID'][rowCount]=sID
                    fitTab['z'][rowCount]=z
                    fitTab['y0'][rowCount]=simTab['y0'][mask][0]
                    fitTab['y1'][rowCount]=simTab['y1'][mask][0]
                    fitTab['x0'][rowCount]=simTab['x0'][mask][0]
                    fitTab['x1'][rowCount]=simTab['x1'][mask][0]
                    squareAreaDeg2=areaMapSqDeg[int(fitTab['y0'][rowCount]):int(fitTab['y1'][rowCount]),
                                                int(fitTab['x0'][rowCount]):int(fitTab['x1'][rowCount])].sum()
                    fracSurveyArea=squareAreaDeg2/areaMapSqDeg.sum()
                    fitTab['areaDeg2'][rowCount]=squareAreaDeg2
                    fitTab['fracSurveyArea'][rowCount]=fracSurveyArea
                    fitTab['MFitSlope'][rowCount]=MFitResult.slope
                    fitTab['MFitIntercept'][rowCount]=MFitResult.intercept
                    fitTab['ycFitSlope'][rowCount]=ycFitResult.slope
                    fitTab['ycFitIntercept'][rowCount]=ycFitResult.intercept  
                    rowCount=rowCount+1
            fitTab.write(outFileName)
            t1=time.time()
            print "... took %.3f sec ..." % (t1-t0)
        else:
            print "... reading fit results for M, yc as function of SNR ..."
            fitTab=atpy.Table().read(outFileName)
        
        # Some applications of selection function...
        SNRCut=parDict['selFnOptions']['fixed_SNR_cut']
        selFn=SelFn.SelFn(rootOutDir, parDict, SNRCut)
        
        # Add detection probabilities for clusters
        SNRStr=('%.1f' % (SNRCut)).replace(".", "p")
        realTab=atpy.Table().read(rootOutDir+os.path.sep+"%s_M500.fits" % (rootOutDir))
        detP=[]
        ycSelectionLimit=[]
        MSelectionLimit=[]
        for row in realTab:
            yc=row['fixed_y_c']*1e-4
            ycErr=row['fixed_err_y_c']*1e-4
            P, ycLimit, MLimit=selFn.detectionProbability(yc, row['redshift'], ycErr, RADeg = row['RADeg'], decDeg = row['decDeg'])
            ycSelectionLimit.append(ycLimit)
            MSelectionLimit.append(MLimit)
            detP.append(P)
        detP=np.array(detP)
        realTab.add_column(atpy.Column(detP, 'detP_SNRCut%s' % (SNRStr)))
        realTab.add_column(atpy.Column(ycSelectionLimit, 'ycLimit_SNRCut%s' % (SNRStr)))  
        realTab.add_column(atpy.Column(MSelectionLimit, 'M500Limit_SNRCut%s' % (SNRStr)))
        outFileName=rootOutDir+os.path.sep+rootOutDir+"_detP.fits"
        if os.path.exists(outFileName) == True:
            os.remove(outFileName)
        realTab.write(outFileName)
                
        # Survey-averaged 90% completeness mass limit and plot
        plotSettings.update_rcParams()
        completenessLim=0.9
        massLimit_90Complete, zRange=selFn.surveyAverage_MLimitAtCompleteness(completenessLim)
        #fontDict={'size': 18, 'family': 'serif'}
        plt.figure(figsize=(9,6.5))
        ax=plt.axes([0.11, 0.12, 0.87, 0.86])
        #plt.tick_params(axis='both', which='major', labelsize=15)
        #plt.tick_params(axis='both', which='minor', labelsize=15)
        tck=interpolate.splrep(zRange, massLimit_90Complete)
        plotRange=np.linspace(0, 2, 100)
        plt.plot(plotRange, interpolate.splev(plotRange, tck), 'k-')
        plt.plot(zRange, massLimit_90Complete, 'D', ms = 8)
        plt.xlabel("$z$")
        plt.ylim(1.5, 10)
        plt.xticks(np.linspace(0, 2, 11), np.linspace(0, 2, 11))
        plt.xlim(0, 2)
        labelStr="$M_{\\rm 500c}$ (10$^{14}$ M$_{\odot}$) [%d" % (int(completenessLim*100.))+"%"+" complete]"
        plt.ylabel(labelStr)
        #plt.title("Survey %d" % (int(completenessLim*100.))+"% "+"completeness limit (area: %.1f deg$^2$)" % (areaMapSqDeg.sum()))
        plt.savefig(diagnosticsDir+os.path.sep+"completenessMLimit_z.pdf")
        plt.close()
        averageMassLimit_90Complete=massLimit_90Complete[np.logical_and(np.greater(selFn.zRange, 0.2), np.less(selFn.zRange, 1.0))].mean()
        print "... survey-average 90 per cent completeness limit (0.2 < z < 1) = %.3f x 10^14 MSun ..." % (averageMassLimit_90Complete)

        # Cumulative area above mass limit plot
        # This needs to be for a fixed z, or averaged over a z range - we'll go with [0.2, 1.0] for now
        selFn.calcMLimits()
        zMin=0.2
        zMax=1.0
        zMask=np.logical_and(np.greater(selFn.fitTab['z'], zMin), np.less(selFn.fitTab['z'], zMax))
        IDs=np.unique(selFn.fitTab['ID'])
        cellMLimits=[]
        cellFracAreas=[]
        for i in IDs:
            idMask=np.equal(selFn.fitTab['ID'], i)
            cellMLimits.append(np.mean(selFn.MLimits[np.logical_and(zMask, idMask)]))
            cellFracAreas.append(np.mean(selFn.fitTab['fracSurveyArea'][np.logical_and(zMask, idMask)]))
        cellMLimits=np.array(cellMLimits)
        cellFracAreas=np.array(cellFracAreas)
        areaTab=atpy.Table()
        areaTab.add_column(atpy.Column(cellMLimits, 'MLimit'))
        areaTab.add_column(atpy.Column(cellFracAreas, 'fracSurveyArea'))
        areaTab.sort('MLimit')
        
        completenessLim=0.5
        #fontSize=18.
        #fontDict={'size': fontSize, 'family': 'serif'}
        plt.figure(figsize=(9,6.5))
        ax=plt.axes([0.11, 0.12, 0.87, 0.86])
        #plt.tick_params(axis='both', which='major', labelsize=15)
        #plt.tick_params(axis='both', which='minor', labelsize=15)
        plt.plot(areaTab['MLimit'], np.cumsum(areaTab['fracSurveyArea']), 'k-')
        plt.ylabel("Fraction of survey area < $M_{\\rm 500c}$ limit")
        plt.xlabel("$M_{\\rm 500c}$ (10$^{14}$ M$_{\odot}$) [%d" % (int(completenessLim*100.))+"%"+" complete]")
        #labelStr="total survey area = %.1f deg$^2$\n$%.1f < z < %.1f$" % (areaMapSqDeg.sum(), zMin, zMax)
        labelStr="total survey area = %.1f deg$^2$" % (areaMapSqDeg.sum())
        plt.ylim(0, 1.05)
        plt.xlim(1.5, areaTab['MLimit'].max())
        plt.figtext(0.4, 0.2, labelStr, ha="left", va="center")
        plt.savefig(diagnosticsDir+os.path.sep+"cumulativeAreaMLimit_0p2_z_1p0.pdf")
        plt.close()
        
        # For folks who like mass limit maps
        if 'makeMLimitMaps' in parDict['selFnOptions'].keys() and parDict['selFnOptions']['makeMLimitMaps'] == True:
            MLimitDir=diagnosticsDir+os.path.sep+"MLimitMaps_SNRCut%.1f" % (SNRCut)
            if os.path.exists(MLimitDir) == False:
                os.makedirs(MLimitDir)
            print "... making MLimit maps ..."
            for z in selFn.zRange:
                print "... z = %.2f ..." % (z)
                selFn.makeM500LimitMap(wcs, z, MLimitDir+os.path.sep+'MLimitMap_z%.2f.fits' % (z))
        
        # Make yc limit map - as expected, these are all the same whichever z you choose (if we averaged over enough points)
        ycLimitMap=selFn.ycLimitMap(wcs, z = 0.4, outFileName = diagnosticsDir+os.path.sep+"ycLimitMap.fits")
        ycLimitMap=ycLimitMap/1e-4
        ycLimitMap=np.ma.masked_where(ycLimitMap == 0, ycLimitMap)
        fontSize=18.0*1.33
        figSize=(22.2, 5.2)
        axesLabels="sexagesimal"
        axes=[0.065,0.17,0.93,0.88]
        fig=plt.figure(figsize = figSize)
        p=astPlots.ImagePlot(ycLimitMap, wcs, cutLevels = [ycLimitMap.min(), np.percentile(ycLimitMap, 99)], title = None, axes = axes, 
                            axesLabels = axesLabels, colorMapName = 'gray', axesFontFamily = 'sans-serif',
                            RATickSteps = {'deg': 15.0, 'unit': 'h'}, decTickSteps = {'deg': 5.0, 'unit': 'd'},
                            axesFontSize = fontSize)
        cbShrink=0.64
        cbAspect=34
        cbLabel="$\\tilde{y}_{0}$ ($10^{-4}$)"
        cb=plt.colorbar(p.axes.images[0], ax = p.axes, orientation="horizontal", fraction = 0.08, pad = 0.22, 
                        shrink = cbShrink, aspect = cbAspect)
        plt.figtext(0.53, 0.045, cbLabel, ha="center", va="center", fontsize = fontSize, family = "sans-serif")
        plt.savefig(diagnosticsDir+os.path.sep+"ycLimitMap.pdf")

        # 2d M500, z versus detection probability plot     
        #print "make yc M500 z sel fn 2d image plot"
        #IPython.embed()
        #sys.exit()
        
        #minMass=5e13
        #areaDeg2=areaMapSqDeg.sum()
        #zMin=0.0
        #zMax=2.0
        #H0=70.
        #Om0=0.30
        #Ob0=0.05
        #sigma_8=0.80
        #mockSurvey=MockSurvey.MockSurvey(minMass, areaDeg2, zMin, zMax, H0, Om0, Ob0, sigma_8)
        #mockSurvey.addSelFn(selFn)
        
        #fontDict={'size': 18, 'family': 'serif'}
        #plt.figure(figsize=(9,6.5))
        #ax=plt.axes([0.09, 0.10, 0.88, 0.88])
        
        
        #plt.tick_params(axis='both', which='major', labelsize=15)
        #plt.tick_params(axis='both', which='minor', labelsize=15)
        #plt.plot(zRange, massLimit_90Complete, 'kD', ms = 8)
        #tck=interpolate.splrep(zRange, massLimit_90Complete)
        #plotRange=np.linspace(0, 2, 100)
        #plt.plot(plotRange, interpolate.splev(plotRange, tck), 'k-')
        #plt.xlabel("$z$", fontdict = fontDict)
        #plt.ylim(2, 10)
        #plt.xticks(np.linspace(0, 2, 11), np.linspace(0, 2, 11))
        #labelStr="$M_{500}$ (10$^{14}$ M$_{\odot}$) [%d" % (int(completenessLim*100.))+"%"+" complete]"
        #plt.ylabel(labelStr, fontdict = fontDict)
        ##plt.title("Survey %d" % (int(completenessLim*100.))+"% "+"completeness limit (area: %.1f deg$^2$)" % (areaMapSqDeg.sum()))
        #plt.savefig(diagnosticsDir+os.path.sep+"completenessMLimit_z.pdf")
        #plt.close()
        
        

        
